# Installations & Setup for Psuedo-Distributed Operation: MapReduce

# Update & Upgrade packages
sudo apt-get update
sudo apt-get upgrade -y

# Install JAVA
sudo apt-get install openjdk-8-jdk
sudo apt-get install openjdk-8-jdk-headless

# Export JAVA_HOME path in .bashrc
nano ~/.bashrc
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
export PATH=${JAVA_HOME}/bin:${PATH}
source ~/.bashrc

# Download Hadoop 3.2.1 from https://hadoop.apache.org/release/3.2.1.html into home, change unzipped file to hadoop directory later in /home/bdhadooprs dirctory

# Modify configuration files for the Pseudo-Distributed Operation from https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation

# Modify etc/hadoop/core-site.xml at the end of file
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

# Modify etc/hadoop/hdfs-site.xml at the end of file
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

# Modify JAVA_HOME in line 55 of etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/

# Install ssh
sudo apt-get install ssh -y

# Install pdsh
sudo apt-get install pdsh -y

# Change PDSH_RCMD_TYPE to ssh, it previously shows rsh when pdsh -q -w localhost command is executed
nano ~/.bashrc
export PDSH_RCMD_TYPE=ssh
source ~/.bashrc

# Check value of Rchmd type if set to ssh
pdsh -q -w localhost

# Setup passphraseless ssh

# Check if you can ssh to the localhost without a passphrase
ssh localhost

# If you cannot ssh to localhost without a passphrase, execute the following commands
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# Again check that you can ssh to the localhost without a passphrase & enter yes
ssh localhost

# Export HADOOP_CLASSPATH & HADOOP_HOME paths in .bashrc
nano ~/.bashrc
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
export HADOOP_HOME=/home/bdhadooprs/hadoop
export PATH=${HADOOP_HOME}/bin:${PATH}


# Setup for YARN on a single node - Psuedo-Distributed Operation: MapReduce

# Modify etc/hadoop/mapred-site.xml at the end of the file

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

# Modify etc/hadoop/yarn-site.xml at the end of the file

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>




# Execution & Useful Commands

# Format the filesystem (During first time installations)
bin/hdfs namenode -format
# The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs)


# Instructions are to run a MapReduce job locally:

# Start NameNode daemon and DataNode daemon
sbin/start-dfs.sh

# List the active JAVA virtual systems or java processes on the target system which are up and running
jps
# SecondaryNameNode, DataNode, Jps, Namenode should be present

# Namenode web interface at default is available at http://localhost:9870/

# Perform hdfs & hadoop operations

# Stop the Namenode daemons and DataNode daemons
sbin/stop-dfs.sh


# Instructions are to run a MapReduce job on YARN:

# Start NameNode, DataNode, NodeManager & ResourceManager
sbin/start-all.sh
(or)
# Start NameNode daemon and DataNode daemon
sbin/start-dfs.sh
# Start ResourceManager daemon and NodeManager daemon
sbin/start-yarn.sh

# List the active JAVA virtual systems or java processes on the target system which are up and running
jps
# SecondaryNameNode, DataNode, Jps, Namenode, ResourceManager, NodeManager should be present

# ResourceManager web interface is available at http://localhost:8088/

# Perform hdfs & hadoop operations

# Stop NameNode, DataNode, NodeManager & ResourceManager
sbin/stop-all.sh
(or)
# Stop the ResourceManager & NodeManager daemons with:
sbin/stop-yarn.sh
# Stop the Namenode daemons and DataNode daemons
sbin/stop-dfs.sh



# Performing hdfs & hadoop operations

# Executing <command> on HDFS
bin/hdfs dfs -<command> local-file-path/ /hdfs-dir-name

# Loading a file into HDFS
bin/hdfs dfs -put absolute-local-file-path/ /hdfs-dir-name

# Running Java MapReduce with Hadoop
bin/hadoop jar path-to-mr-jar-file.jar /input-dir-path /output-dir-path

# Running Python MapReduce with Hadoop Streaming
bin/hadoop jar absolute-streaming-jar-file-path.jar \
-input /hdfs-input-dir-path \
-output /hdfs-output-dir-path \
-mapper absolute-path-to-mapper/mapper.py \
-reducer absolute-path-to-reducer/reducer.py


